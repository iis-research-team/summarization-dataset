Введение
Уменьшение размерности – одна из основных частей процесса анализа данных. Низкоразмерные данные могут быть эффективно визуализированы. Такие визуализации полезны в исследовательском анализе данных для изучения их природы. Меньшим количеством признаков
удобно манипулировать и анализировать человеку. Сокращение размерности полезно при обработке больших данных, поскольку оно уменьшает объем данных с минимально возможной
потерей информации и позволяет ускорить обработку больших данных [1].
Очень важно, чтобы признаки, генерируемые алгоритмами уменьшения размерности,
были интерпретируемыми в таких областях, как медицина, финансы, управление персоналом,
правосудие, образование и маркетинг. Другими словами, все области, где решения могут повлиять на человека не лучшим образом с опасностью для здоровья, жизни, финансов, нуждаются в интерпретируемых манипуляциях с данными [2, 3]. Эксперт должен уметь объяснить
значение генерируемых признаков в терминологии предметной области. В противном случае
решение, предоставляемое алгоритмом, не может быть использовано в производстве, поскольку отсутствие объяснимости приводит к непредвиденным ошибкам, которые недопустимы
в таких предметных областях.
Проблему уменьшения размерности можно представить как поиск низкоразмерного представления исходного пространства. Эта идея лежит в основе Manifold Learning [4, 5], которая
предполагает, что полезные данные содержатся в низкоразмерном многообразии, вложенном
в высокоразмерное пространство. Таким образом, цель состоит в том, чтобы найти представление, которое сохраняет полезную информацию и не содержит бесполезной информации.
Полезность информации может быть определена многими способами. Самый прямой способ – это экспертная оценка информации. В то же время это самый сложный способ, потому
что он требует наличия эксперта, которого трудно найти. Кроме того, время специалиста стоит
дорого. Также существует естественное ограничение на объем информации, который может
оценить человек. Таким образом, экспертная оценка подходит в отдельных случаях данных
с небольшим количеством признаков. В случае с автоматической оценкой полезности информации существует широкий спектр вариантов. Это может быть качество предсказания модели
машинного обучения, линейная разделимость [6] кластеров, принадлежащих разным классам,
мера расстояния между такими кластерами или какая-либо пользовательская бизнес-метрика,
которая может быть рассчитана на данных.
Генетическое программирование [7–9] – это подход к решению задач оптимизации. У него
есть несколько важных свойств, которые отличают этот тип оптимизации от других. В генетической оптимизации существует функция, называемая фитнес, которую необходимо оптимизировать. Требования к фитнес-функции гораздо мягче, чем, например, к функции, оптимизируемой при градиентном спуске [10]. Фитнес-функция может быть недифференцируемой, это
самое интересное свойство генетической оптимизации, позволяющее легко создавать сложные
фитнес-функции, совершенно не заботясь об их дифференцируемости. Существуют модификации генетической оптимизации, которые не требуют даже точного значения фитнес-функции. Таким алгоритмам нужны только ранги, т. е. отношения «больше-меньше» между фитнес-функциями индивидов, для сравнения их друг с другом. В этом случае лучшее решение
имеет самый высокий (или самый низкий) ранг, но точное значение функции при этом может
быть неизвестно.
Чтобы построить решение, необходимо выбрать такое представление признаков, которое
может эффективно мутировать. Символическая (или символьная) регрессия [11, 12] – это подход к нахождению арифметического выражения, которое описывает закон, по которому производятся данные. Он может быть представлен в виде дерева выражений, где лист – это терминал
(признак из исходных данных), а узел – операция над дочерними поддеревьями. Деревья могут быть закодированы различными способами: графы [13], обратная польская нотация [14],
или менее известные методы, такие как генные выражения [15, 16], которые и используются
в данной работе и будут рассмотрены далее.
В этой работе мы предлагаем прозрачный алгоритм уменьшения размеров под названием
ГУРу (Генетическое Уменьшение Размерности). ГУРу является эволюционным алгоритмом
и реализует общий конвейер генетического алгоритма. Он строит признаки, стремясь обеспечить линейную разделимость пространства признаков и максимизировать среднее расстояние
между объектами различных классов.
Обзор существующих работ
Классические подходы строят непрозрачные решения, потому что они оперируют данными с позиции статистики и не используют никаких знаний о предметной области. Например,
PCA [17] находит подпространство в исходном пространстве, в котором вдоль осей у данных
наибольшая дисперсия. Для аналитика данных является сложной задачей объяснить значение
таких признаков с точки зрения предметной области. Наиболее распространенным случаем
решения задачи сокращения размерности является использование классических подходов, таких как PCA, когда признаков очень много и работать с таким большим количеством признаков
просто невозможно, или t-SNE [18], когда необходимо сделать некоторые интуитивные выводы о распределении данных путем визуализации пространства признаков. С другой стороны,
существует несколько работ, посвященных прозрачному сокращению размерности [19–22].
Они используют эволюционный подход для уменьшения размерности данных и получения интерпретируемых человеком признаков. Эти алгоритмы генерируют признаки в виде деревьев
выражений, которые имеют тот же уровень интерпретируемости, что и исходные признаки,
поскольку операции, применяемые к данным, легко интерпретировать. Поэтому, если имеется
большой объем данных, который необходимо уменьшить без потери смысла, эти алгоритмы
можно использовать вместо классических подходов, которые не могут уменьшить размерность
данных без потери интерпретируемости и генерируют признаки, которые трудно объяснить
с помощью терминологии предметной области.
Алгоритм GP-DR
В работе [20] авторы исследуют, насколько хорошо работает простой генетический алгоритм без настройки гиперпараметров (размер популяции и количество эпох) в задаче сокращения размерности. В статье авторы не дают ему названия, поэтому в данной работе назовем
этот алгоритм GP-DR (Genetic Programming for Dimension Reduction). Существует несколько
фитнес-функций: расстояние между объектами (А), сохранение ранга (на основе расстояния)
(Б), функция, оценивающая качество дистилляции кодирующей части автоэнкодера (В), и качество дистилляции всего автоэнкодера (Г). Во всех случаях это алгоритм обучения без учителя.
Алгоритм (А) сравнивает расстояния между объектом в исходном высокоразмерном пространстве и его проекцией в сгенерированном низкоразмерном пространстве. Сумма таких
расстояний должна быть минимизирована. В этом заключается суть фитнес-функции, основанной на расстоянии. Фитнес-функция, сохраняющая ранг (Б), предназначена для сохранения порядка рангов
расстояний. Для каждого объекта все остальные объекты могут быть отсортированы по расстоянию и ранжированы. В новом пространстве эти ранги должны быть такими же, как в исходном пространстве. Эта фитнес-функция предназначена для того, чтобы генетический алгоритм минимизировал количество неправильно упорядоченных пар объектов в низкоразмерном
пространстве. Объекты с более высокими рангами более важны, чем объекты с более низкими
рангами. Таким образом, в фитнес-функции используется взвешивание объектов в соответствии с их рангами.
Другая фитнес-функция (В) использует выходной сигнал кодирующей части автоэнкодера
и предназначена для того, чтобы генетический алгоритм генерировал такой же выходной сигнал. Ее можно представить как задачу символьной регрессии на выходе кодировщика и исходном пространстве признаков в качестве цели.
Последняя фитнес-функция (Г) реализует тот же подход, но выход автоэнкодера в целом
должен сравниваться с выходом генетического алгоритма. Деревья генетических алгоритмов
делятся на деревья кодировщиков и деревья декодировщиков. При этом только кодирующие
деревья используются для генерации низкоразмерного пространства после завершения подгонки алгоритма.
Алгоритм GP-MaL-MO
Этот генетический алгоритм основан на идее, что соседи в новом низкоразмерном пространстве должны иметь порядок, аналогичный порядку в исходном высокоразмерном пространстве. Этот алгоритм называется GP-MaL-MO (Genetic Programming for Manifold Learning
using a Multi-objective Approach) [21], является расширением алгоритма GP-MaL [23]. Данный
алгоритм использует многокритериальную функцию пригодности для построения фронта Парето из решений. Он использует многоцелевой эволюционный алгоритм на основе декомпозиции (MOEA/D) [24] для генерации решений. Алгоритм также генерирует решения в виде
деревьев выражений с исходными признаками в листьях и операциями между ними в узлах
дерева. Таким образом, GP-MaL-MO генерирует деревья выражений, которые могут быть интерпретированы человеком и могут быть использованы как прозрачный алгоритм уменьшения
размерности.
Существует компромисс между качеством и количеством признаков (измерений). Таким
образом, пользователь может выбрать решение, удовлетворяющее его требованиям. Алгоритм
имеет операторы кроссинговера и мутации, разработанные для того, чтобы GP-MaL-MO мог
генерировать решения как с одним деревом, так и со ста деревьями, что приводит к богатому
фронту Парето на выходе алгоритма. Как показывают авторы, алгоритм работает так же хорошо, как и классические алгоритмы уменьшения размерности, такие как PCA, LLE и др. [25],
MDS [26] и UMAP [27], и полученные с его помощью решения имеют сопоставимое качество.
Описание алгоритма ГУРУ
В ГУРУ реализовано несколько отличительных особенностей:
• Хромосома – это дерево вычислений, которое кодируется методом «генных выражений» [15, 16].
• Фитнес-функция является динамической и изменяется для каждого конструируемого
признака.
• Конвейер алгоритма разделен на две части: фаза исследования и фаза эксплуатации.
Все эволюционные алгоритмы имеют схожую конструкцию. Это цикл, начинающийся
с оценки стартовой популяции, выбора лучших особей, мутации, кроссинговера особей и создания новой популяции из их детей. Эта процедура повторяется, пока не срабатывают критерии остановки. Выбор генетических операций влияет на стиль поиска алгоритма в пространстве поиска, его скорость и баланс между поведением разведки и эксплуатации.
Далее приводится подробное описание генетических операций и операндов в ГУРУ.
ГУРУ работает с индивидами, которые представляют собой деревья вычислений. Эти деревья содержат терминальные признаки в листьях и операции в узлах. Таким образом, если
это дерево будет вычисляться, то на входе будет несколько терминалов, а на выходе – одна
функция.
Алгоритм работает с числовыми данными, представленными столбцами входного датафрейма данных. Текущая реализация работает с датафреймами известной в среде аналитиков данных библиотеки pandas. Таким образом, столбцы исходного набора данных, содержащие числовые данные, являются терминальными признаками для алгоритма.
Популяция в ГУРУ – это массив определенного размера, содержащий индивидов. Ее размер меняется между фазами разведки и эксплуатации, но является константой для всех итераций в одной фазе.
В версии ГУРУ, представленной в данной работе, используются обычные арифметические операции из табл. 1. Они могут быть легко интерпретированы аналитиком (при условии,
что в конкретной предметной области эти операции между признаками имеют смысл).
В алгоритме используются три типа мутации: инверсия, транспонирование последовательности вставки (IS), транспонирование корневой последовательности вставки (RIS).
• Инвертирующая мутация случайным образом выбирает подпоследовательность в массиве head и инвертирует ее.
• Мутация IS Transpose случайным образом выбирает подпоследовательность внутри
головки и вставляет ее в другую позицию, кроме начальной.
• Мутация RIS Transpose делает то же самое, что и IS Transpose. Единственное отличие
заключается в том, что RIS Transpose вставляет подпоследовательности только из корневой позиции.
В ГУРУ используются стратегии одноточечного и двухточечного кроссинговера.
• Одноточечный кроссинговер берет два гена, случайным образом выбирает точку и создает двух потомков, объединяя части родителей относительно выбранной точки.
• Двухточечный кроссинговер похож на одноточечный, но в нем выбираются две точки,
и потомки строятся путем обмена материалами родителей между этими двумя точками.
В ГУРУ лучшие индивиды отбираются с помощью турнирного отбора. В нем случайным
образом выбирается фиксированное число особей и сравниваются их показатели приспособленности. Лучшая особь из каждого турнира переходит в следующее поколение.
ГУРУ оценивает новые признаки по отношению к ранее созданным. Это заставляет алгоритм находить такие новые признаки, которые содержат информацию, не содержащуюся в ранее созданных признаках. Это приводит к построению выразительных компактных признаков.
Стоит отметить, что каждый новый признак сам по себе все менее и менее важен, чем ранее
сгенерированный, поскольку каждый новый признак зависит от всех предыдущих. Поэтому
возможно, что какой-то признак высокого ранга (сгенерированный на поздней эпохе) не содержит никакой полезной информации для классификации, но содержит информацию об ошибках
всех предыдущих признаков. Это может привести к чрезмерной подгонке (переобучению).
Первый признак оценивается с помощью подгонки к нему функции пригодности и 3-кратной кросс-валидации. Каждый следующий признак оценивается с помощью ранее созданных
признаков. Например, второй признак оценивается с помощью оценивания функции пригодности на данных, которые содержат первый признак, полученный из предыдущей эпохи. Это
позволяет ГУРУ генерировать признаки таким образом, что каждый новый признак исправляет недостатки всех ранее сгенерированных признаков. Данный процесс проиллюстрирован
на рис. 1.
Программирование выражений генов – это разновидность генетического программирования. Отличительной особенностью генных выражений является линейное представление фиксированной длины. Такое представление может быть преобразовано в вычислительное дерево.
Хромосома в генном выражении – это строка фиксированной длины. Она делится на головку
и хвост, как показано на рис. 2. Голова – это список функций и терминалов постоянной длины.
Хвост содержит только терминалы. Его длина зависит от того, сколько операндов требуется головным функциям. Листья дерева – это терминалы, а узлы – функции. Преимуществом генных
выражений является простота реализации генетических операций (мутация, кроссинговер,
оценка). Недостатком может быть недостаточная выразительность выражений из-за фиксированной длины, что с другой стороны также сокращает пространство поиска решений. Выражение гена однозначно отображается в дерево вычислений, как показано на рис. 3.
Однако одно дерево может отображаться на более чем одно генное выражение, поскольку существуют генные выражения, содержащие неиспользуемые операции и терминалы.
Генные выражения – это один из многих методов кодирования дерева вычислений.
Он прост в использовании и управлении: регулируя длину генного выражения, мы можем избежать чрезмерной подгонки и регулировать выразительность решения.
Функция пригодности оценивает качество особи. Это одна из самых важных частей генетического алгоритма, поскольку она определяет пространство поиска. Очень важно сделать
фитнес-функцию, которая описывает решения с разных точек зрения, чтобы уменьшить вероятность перебора. В данной работе мы используем динамическую многокритериальную фитнес-функцию.
Многокритериальная фитнес-функция использует более одного критерия для оценки
особи. В данной работе мы используем два критерия для каждой итерации генерации. Фитнес-функция представлена взвешенной суммой, как в (1). Один из критериев называется примесью, а другой – базовой фитнес-функцией. Базовая функция не изменяется в процессе работы алгоритма и является одной и той же для всех итераций алгоритма. Примесь изменяется
на каждой итерации. В ГУРУ мы используем такую конфигурацию мультикритериальной фитнес-функции:
• Базовая функция – линейная машина опорных векторов [28].
• Примесь – это один из трех алгоритмов: логистическая регрессия, дерево решений
или мера расстояния.
Интуиция, лежащая в основе такой мультикритериальной функции, заключается в том,
чтобы построить различные признаки благодаря динамической примеси. И в то же время признаки должны строить линейно разделяемое пространство благодаря статической базовой
функции, которая является линейной моделью. Такая комбинация приводит к стремлению алгоритмом создать линейно разделимое пространство даже при наличии нелинейной функциипримеси. Фитнес-функция является динамической, поскольку алгоритм оценивает каждый новый
признак с помощью другой фитнес-функции. Пример сокращения до двух измерений показан на рис. 1. Это делает пространство признаков результата более разнообразным и выразительным по сравнению со статической фитнес-функцией, когда существует одна и та же
модель для оценки всех генерируемых признаков. В данной работе используются простые
фитнес-функции из разных семейств алгоритмов:
• Логистическая регрессия, которая должна заставить алгоритм построить признак, делающтй пространство более линейно разделяемым.
• Мера расстояния, приводящая к построению признака, который смещает объекты
разных классов на большее расстояние. Мы используем среднее попарное расстояние между объектами классов. Оно масштабируется по формуле (2) таким образом,
что значение около 0,0 является наихудшим, а 1,0 – наилучшим: Дерево решений. Это простая и быстрая модель, принцип работы которой сильно отличается от логистической регрессии и меры расстояния. Лучше использовать признаки,
полученные с помощью нелинейной модели, чем с помощью другой линейной модели.
Компромисс между разведкой и эксплуатацией – известная дилемма в машинном обучении. Исследование – это такое поведение алгоритма, когда он совершает большие хаотические
скачки в пространстве поиска и потенциально имеет возможность выскочить из локального
экстремума и попасть в другой. Такое поведение увеличивает шансы найти глобальный экстремум. Тем не менее, разведка является плохой стратегией для поиска точной точки экстремума.
Напротив, в случае поведения эксплуатации алгоритм делает небольшие шаги в пространстве
поиска. Это помогает найти точную точку экстремума. Однако эксплуатирующий алгоритм,
скорее всего, не найдет ни глобального, ни даже другого локального экстремума, а будет медленно и верно сходиться к тому экстремуму, возле которого находится.
Генетические алгоритмы также страдают от компромисса между разведкой и эксплуатацией. Популяция может содержать похожие особи, которые могут иметь малые вероятности мутации и кроссинговера. Алгоритм с такой конфигурацией будет реализовывать стратегию эксплуатации, поскольку особи меняются медленно. Поэтому алгоритм делает небольшие шаги
в пространстве поиска. В других случаях вероятности мутации и кроссинговера достаточно
высоки. Тогда алгоритм будет генерировать различные особи в популяции. Это поведение разведки, поскольку каждая особь имеет небольшой шанс выжить и дать потомство с похожими свойствами. Возможно, она мутирует и потеряет свои полезные свойства или потеряет их
при скрещивании с другой особью.
В данной работе мы разделили процесс работы алгоритма на две фазы: разведка и эксплуатация. Идея заключается в том, чтобы дать алгоритму свободу в начале процесса поиска
генерировать огромное количество различных особей. Они будут отсортированы по значению
фитнес-функции, и алгоритм возьмет подмножество лучших особей. Затем алгоритм начнет
фазу эксплуатации.
Сбор данных
Бенчмарк (набор данных для оценки качества работы алгоритмов) содержит 12 датасетов,
загруженных с сайта проекта OpenML [29]: bank marketing [30], blood transfusion [31], breast
cancer wisconsin [32], [33], credit-g [33], diabetes [33], hyperplane [29], ionosphere [34], madelon
[35], sonar [36], bioresponse [29], christine [29], guillermo [29]. Была протестирована только задача бинарной классификации. Как видно из табл. 2, все наборы данных в целом имеют числовые признаки и только два набора данных имеют категориальные признаки, чтобы посмотреть, как алгоритмы будут работать на наборах данных, где важны категориальные признаки.
Для проверки устойчивости алгоритмов есть несколько достаточно больших наборов данных,
таких как hyperplane и guillermo. Баланс классов также различается, но нет наборов данных
с огромным дисбалансом.
В наборе данных christine имеется 38 бинарных и унарных признаков. В данной работе
мы используем их как числовые признаки, где False – 0.0, а True – 1.0.
В табл. 2 в колонке OpenML id представлены идентификаторы наборов данных на сайте. Эксперимент
В этом разделе представлено сравнение с другими прозрачными алгоритмами уменьшения
размерности. Все эксперименты запускались десять раз с различными случайными семенами
из фиксированного набора. В бенчмарке наборы данных разбиваются на обучающий и тестовый наборы. Затем каждый алгоритм обучается на обучающем множестве. Настроенная модель преобразует как обучающий, так и тестовый наборы. Затем модель классификации обучается на преобразованном обучающем множестве и оценивается на преобразованном тестовом
множестве. Используется метрика AUC-ROC. Все десять результатов, рассчитанных для каждого алгоритма на каждом наборе данных, усредняются, и этот средний балл используется
для сравнения алгоритмов.
Конфигурация ГУРУ была одинаковой во всех бенчмарках. Она представлена в табл. 3.
Гиперпараметры вероятности кроссинговера и мутации были предварительно настроены
на бенчмарке. Все остальные гиперпараметры были определены интуитивно. Таким образом,
возможно, что ГУРУ может достичь лучшего качества в другой конфигурации, что является
предметом отдельного исследования. GP-DR и GP-MaL-MO были адаптированы для выполнения в нашем бенчмарке (это означает, что они должны реализовать интерфейс трансформера из библиотеки sklearn). После
адаптации они были запущены в бенчмарке с конфигурациями по умолчанию. Единственное
отличие в гиперпараметрах – это количество выходных признаков, которое было изменено
на 2 там, где это было возможно.
Результатом GP-MaL-MO является фронт Парето с компромиссом между качеством решения и количеством измерений. В этом эксперименте использовано лучшее решение с двумя
измерениями, если оно существовало, и объединение двух решений с одним измерением в противном случае.
В этом эксперименте мы сравниваем метрики линейной модели классификации (логистической регрессии), обученной и протестированной на признаках, сгенерированных различными алгоритмами уменьшения размерности. Все алгоритмы строят двумерное пространство
признаков, поскольку это наиболее распространенный случай в реальной жизни, когда уменьшение размерности используется для того, чтобы сделать пространство признаков пригодным
для визуализации (трехмерное пространство также может быть визуализировано, но построить понятную изометрическую визуализацию – более сложная задача). Линейная модель используется потому, что качество ее предсказаний показывает, насколько хорошо пространство
признаков может быть линейно разделено. Линейно разделяемое пространство удобно для визуального анализа. Кроме того, линейные зависимости в целом более интерпретируемы и понятны человеку.
Генетические алгоритмы преобразуют пространство признаков с помощью преобразования, имеющего наибольшее значение фитнес-функции.
GP-DR не очень хорошо работает с достаточно большими наборами данных. Именно поэтому некоторые результаты для этого алгоритма отсутствуют. GP-MaL-MO также не может
завершить вычисления на больших наборах данных. В качестве базового решения в этом эксперименте использовался отбор признаков с помощью линейной модели (величина коэффициентов логистической регрессии), поскольку выбор
признаков можно рассматривать как примитивное прозрачное уменьшение размерности.
Анализ результатов
Как видно из табл. 4, ГУРУ превосходит другие генетические алгоритмы в этом эксперименте почти на всех наборах данных. Однако стоит отметить результаты на наборе данных
credit-g, где ГУРУ показывает худший результат среди всех алгоритмов. Причиной этого, вероятно, является нетривиальная комбинация числовых и категориальных признаков, содержащих важную для классификации информацию. ГУРУ генерирует признаки без каких-либо знаний о категориальных признаках. Это приводит к эффектам, подобным этому, на тех наборах
данных, где важна комбинация числовых и категориальных признаков.
Вероятно, такая же ситуация и с набором данных bank-marketing, поскольку в случае трехмерного пространства качество значительно лучше. В случае с набором данных sonar трудно
делать предположения, потому что этот набор данных небольшой и имеет всего 208 объектов,
и несколько других алгоритмов на таком малом датасете вообще не смогли успешно отработать.
Также стоит отметить, что ГУРУ – единственный из генетических алгоритмов смог успешно отработать на больших по количеству признаков датасетах guillermo и hyperplane.
Заключение
В этой работе мы попытались создать прозрачное решение для уменьшения размерности и представили ГУРУ. Алгоритм обеспечивает построение линейно сепарабельного низкоразмерного пространства признаков посредством конструирования признаков. Он работает
с числовыми признаками, а сгенерированные признаки представлены в виде выражений генов, являющихся деревьями выражений, которые могут быть интерпретированы человеком.
ГУРУ использует динамическую многокритериальную фитнес-функцию для оценки особей,
которая позволяет строить разнообразные решения. Чтобы справиться с компромиссом между
исследованием и эксплуатацией, конвейер алгоритмов разделен на две фазы: короткая фаза
исследования с популяциями большого размера в начале и длинная фаза эксплуатации со значительно меньшими популяциями в дальнейшем. Эксперименты показали, что ГУРУ обеспечивает лучшее качество классификации по линейной модели среди аналогов в случае уменьшения размерности до двух измерений. Стоит отметить, что ГУРУ может быть использован
как алгоритм инженерии признаков, который строит признаки, обеспечивающие высокое качество классификации и поддающиеся интерпретации. Он может быть использован аналитиком
для генерации большого набора различных признаков и поиска среди них значимых, которые
могут привести к пониманию данных. Исследование такого применения является одним из направлений будущей работы.